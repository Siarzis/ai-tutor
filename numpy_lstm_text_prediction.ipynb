{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfnJrhoCXO8yAQmpM3+lOn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Siarzis/ai-tutor/blob/main/numpy_lstm_text_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries"
      ],
      "metadata": {
        "id": "2p96vqoHr57j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "v0r7AtR4rvZY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a simple dataset of text sequences"
      ],
      "metadata": {
        "id": "LE7iUrCrr_mU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "def generate_dataset(num_words=100, max_word_size=10):\n",
        "\n",
        "    sequences = []\n",
        "\n",
        "    for _ in range (num_words):\n",
        "\n",
        "        random_number =  np.random.randint(1, max_word_size)\n",
        "        sample = ['s'] * random_number + ['t'] * random_number +  ['EOS']\n",
        "        sequences.append(sample)\n",
        "\n",
        "    return sequences\n",
        "\n",
        "num_words=100\n",
        "max_word_size=10\n",
        "\n",
        "vocabulary = generate_dataset(num_words, max_word_size)\n",
        "print(vocabulary[88])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtDSH0h7sEYu",
        "outputId": "abbaf313-d032-4bb6-c372-ae519571da2a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['s', 's', 's', 's', 's', 's', 's', 's', 't', 't', 't', 't', 't', 't', 't', 't', 'EOS']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correspond Vocabulary Tokens to Indices"
      ],
      "metadata": {
        "id": "EjugjcoeJR1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokens_to_indices(vocabulary):\n",
        "\n",
        "    token_to_index = {}\n",
        "    index_to_token = {}\n",
        "\n",
        "    # flatten the given vocabulary to convert it into tokens\n",
        "    flatten_vocabulary = sum(vocabulary, [])\n",
        "\n",
        "    tokens = list(set(flatten_vocabulary))\n",
        "\n",
        "    # append the unknown-character token\n",
        "    tokens.append('UNK')\n",
        "\n",
        "    for i, t in enumerate(tokens):\n",
        "        token_to_index[t] = i\n",
        "        index_to_token[i] = t\n",
        "\n",
        "    return token_to_index, index_to_token\n",
        "\n",
        "tokens_to_indices(vocabulary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPwFv-2NIz4j",
        "outputId": "37ced4af-757c-45a6-fa3f-ade62c750e2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'t': 0, 'EOS': 1, 's': 2, 'UNK': 3}, {0: 't', 1: 'EOS', 2: 's', 3: 'UNK'})"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform one-hot encoding on the generated vocabulary"
      ],
      "metadata": {
        "id": "WHjhHgixBqwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seq_to_dict(vocabulary):\n",
        "\n",
        "    word_to_index = {}\n",
        "    index_to_word = {}\n",
        "\n",
        "    str_vocabulary = [''.join(word) for word in vocabulary]\n",
        "\n",
        "    # remove duplicate words in the dictionary\n",
        "    unique_vocabulary = set(str_vocabulary)\n",
        "    unique_vocabulary = list(unique_vocabulary)\n",
        "\n",
        "    # find the word with the maximum length in the vocabulary\n",
        "    max_word_size = max(unique_vocabulary, key=len)\n",
        "    # compute the maximum word length\n",
        "    max_word_size = int((len(max_word_size) - 3 ) / 2)\n",
        "\n",
        "    # create a diagonal list of lists\n",
        "    encoding = [['0' if i != j else '1' for j in range(max_word_size)] for i in range(max_word_size)]\n",
        "    encoding = [''.join(row) for row in encoding]\n",
        "\n",
        "    for word in unique_vocabulary:\n",
        "\n",
        "        word_length = int((len(word) - 3 ) / 2)\n",
        "        word_to_index[word] = encoding[word_length-1]\n",
        "        index_to_word[encoding[word_length-1]] = word\n",
        "\n",
        "    return word_to_index, index_to_word\n",
        "\n",
        "word_to_index, index_to_word = seq_to_dict(vocabulary)\n",
        "\n",
        "print(word_to_index)\n",
        "print(index_to_word)\n",
        "\n",
        "index_to_word = 1"
      ],
      "metadata": {
        "id": "_87B5ukiBsXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleSequencesDataset(Dataset):\n",
        "    def __init__(self, data, targets):\n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, id):\n",
        "        sample = {'data': self.data[id], 'target': self.targets[id]}\n"
      ],
      "metadata": {
        "id": "ArXSUFbSXxWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialization of the RNN"
      ],
      "metadata": {
        "id": "Apw9WbJArC-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn_init(num_hidden_nodes, num_tokens):\n",
        "\n",
        "    # initialize input weight matrix\n",
        "    W_x = np.random.uniform(-1, 1, size=(num_hidden_nodes, num_tokens))\n",
        "\n",
        "    # initialize hidden state weight matrix\n",
        "    W_h = np.zeros((num_hidden_nodes, num_hidden_nodes))\n",
        "\n",
        "    # initialize output weight matrix\n",
        "    W_y = np.random.uniform(-1, 1, size=(num_tokens, num_hidden_nodes))\n",
        "\n",
        "    return W_x, W_h, W_y\n",
        "\n",
        "rnn_init(3, 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xS_lHAiFrBkt",
        "outputId": "d21e37a2-9140-4714-ec8d-bc0dc833c61a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 0.33797651,  0.16137324, -0.25543447,  0.88026688],\n",
              "        [ 0.94732767, -0.43215805, -0.38927228, -0.02877249],\n",
              "        [-0.10315171,  0.98891493, -0.64814949, -0.96384927]]),\n",
              " array([[0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.]]),\n",
              " array([[-0.01221257, -0.64235458, -0.26706243],\n",
              "        [ 0.48834105,  0.44187985, -0.38387842],\n",
              "        [ 0.08508046,  0.01762815,  0.27266524],\n",
              "        [-0.49907636,  0.1797417 ,  0.95778572]]))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition of activation functions"
      ],
      "metadata": {
        "id": "0rkwHVhMEz6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    # calculate the exponential of each element in the array\n",
        "    exp_x = np.exp(x)\n",
        "\n",
        "    # calculate the sum of the exponentials\n",
        "    sum_exp_x = np.sum(exp_x)\n",
        "\n",
        "    # calculate the softmax values by dividing each exponential by the sum of exponentials\n",
        "    softmax_values = exp_x / sum_exp_x\n",
        "\n",
        "    return softmax_values"
      ],
      "metadata": {
        "id": "2AblMZWuE4HS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward pass"
      ],
      "metadata": {
        "id": "j0ZRe4qRBeTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn_forward_pass():\n",
        "\n",
        "    # calculate hidden nodes (a_t)\n",
        "    a_t = W_h @ h_t_1 + W_x @ x_t\n",
        "\n",
        "    # calculate hidden state\n",
        "    h_t = np.tanh(a_t) # TO CHECK\n",
        "\n",
        "    # calculate RNN output\n",
        "    # softmax maps a given vector to range [0.0, 1.0]. Since we have a one-hot\n",
        "    # encoding representation, we need to find which index of the generated\n",
        "    # output has the largest probability to be predicted\n",
        "    y_t = softmax(W_y @ h_t)\n",
        "\n"
      ],
      "metadata": {
        "id": "Lyg16gj6CBsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "N3J_5__AGoeL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}